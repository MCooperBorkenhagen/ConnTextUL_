---
title: "Proof of Concept (Oct 2023)"
output: html_document
date: "2023-10-27"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview
These are results from proof of concept trials for the transformer architecture. Test data are from the WJIII. These data were compiled in simulations run by NC in November, 2023 and draw from the dataset `102923_outputs.xlsx`. These data represent the version of the simulations run over all the training words, which can be found in `102923_inputs.csv`.


### Processing scripts
You can find processing scripts for these data in `scripts/102923_data.R` and `102923_global_encoding.R`. There are some differences between these data (the objects for which are noted above) and the data from other runs - so just beware of that.
```{r Setup}
require(tidyverse)
conflicted::conflicts_prefer(dplyr::filter)

lapply(read.csv('scripts/requirements.txt', stringsAsFactors = F)[[1]], require, ch = T)
source('scripts/utilities.R')
here()

# k-smoothed to value of 2
tasa = read_csv('../words/tasa/tasa.csv') %>% 
  mutate(word = tolower(word)) %>% 
  group_by(word) %>% 
  summarise(frequency = sum(frequency)) %>% 
  mutate(frequency = case_when(is.na(frequency) ~ 2,
                               TRUE ~ frequency + 2)) %>% 
  select(word, tasa = frequency)

phonreps = read_csv('data/phonreps.csv')

elp = read_csv('../words/elp/elp_full_5.27.16.csv') %>% 
  select(word = Word, acc = I_Mean_Accuracy, naming = I_NMG_Mean_RT, ld = I_Mean_RT) %>% 
  mutate(word = tolower(word),
         ld = as.numeric(ld))

inputs = read_csv('data/102923_data.csv') %>% 
  select(word = word_raw, program_name, program_type)

# these predictions do not include words that are only present in
# the training set (they do not include words that are only in WJ3)
preds_by_phoneme = read_csv('data/102923_preds_by_phoneme.csv')

# these predictions do not include words that are only present in
# the training set (they do not include words that are only in WJ3)
preds_by_word = read_csv('data/102923_preds_by_word.csv')
# to fix the missing data above, we need to get the target representations
# for the words in WJ3 but not in training set


# accuracy by word data
# these data DO include words that are in WJ3 but not in training
# set, unlike the preds_ data above which do not
path = 'data/102923_outputs.xlsx'
sheets = excel_sheets(path)


# the main dataframe
d = read_excel(path, sheet = sheets[1]) %>% 
  mutate(epoch = str_replace(sheets[1], "epoch 0", ""))

for (sheet in sheets[-1]) {
  
  tmp = read_excel(path, sheet = sheet) %>% 
    mutate(epoch = str_replace(sheet, "epoch 0", ""))
  d = rbind(d, tmp)
  
}

include = c("correct", "in_traindata", "epoch")


d = d %>% 
  filter(word_raw %nin% c("r", "m", "b", "k", "h", "t")) %>% # WJ3 items that aren't informative
  mutate(epoch = as.numeric(epoch),
         correct = as.numeric(correct)) %>% 
  select(word = word_raw, in_validation_set = in_wj3, one_of(include)) %>% 
  left_join(inputs %>% 
              group_by(word) %>% 
              summarise(f = n())) %>% 
  left_join(read_csv('../words/consistency/chee_consistency.csv') %>% 
              select(word, consistency = ff_all_r)) %>% 
  mutate(consistency = case_when(is.na(consistency) ~ 1,
                                 TRUE ~ consistency)) #%>% 
#  left_join(preds_by_word %>% # return to this when we have target patterns for all the WJ3 words
#              select(word, epoch, phon_distance_from_target), by = c("word", "epoch"))

# these data add in the frequency of words in each program
# (this facilitates certain anayses below). Also a frequency of
# zero is included when NA results from the join
# note that the programs themselves aren't included in this
# we don't do those analyses here
programs = d %>% 
  select(epoch, word, correct,  in_validation_set, in_traindata) %>% 
  left_join(inputs %>% 
              group_by(program_type, word) %>% 
              summarise(f = n()), relationship = "many-to-many") %>% # because a given word in d is associated with more than one program type
  mutate(f = case_when(is.na(f) ~ 0,
                       TRUE ~ f))




#WJ3 sequence
wj3_sequence = read_csv('data/wj_iii_form_a.csv') %>% 
  select(sequence, word = orth) %>% 
  filter(word %in% d$word) %>% 
  left_join(d %>% 
              filter(epoch == 20) %>% 
              select(word, in_traindata, in_validation_set))

rm(path, sheets)
```

### Learning of all words across epochs
For words in the training set the model achieves generally high learning, but not all words are learned perfectly. Some of these words are in the training data and others are not. There is basically no generalization to the WJ3 words that aren't in the training set.
```{r}
d %>% 
  group_by(epoch, word) %>% 
  summarise(accuracy = mean(correct),
            in_validation_set = first(in_validation_set),
            in_traindata = first(in_traindata)) %>% 
  filter(in_validation_set) %>% 
  ggplot(aes(epoch, accuracy, color = in_traindata)) +
  geom_smooth() +
  theme_apa() +
  facet_grid(vars(in_traindata)) +
  labs(x = "Epoch", y = "Accuracy", color = "In training data")

```

There are 36 words that are in the validation set but not in the training set.


```{r}
d %>% 
  filter(!in_traindata & in_validation_set) %>% 
  ggplot(aes(epoch, correct)) +
  geom_smooth() +
  theme_apa()  +
  labs(x = "Epoch", y = "Accuracy", title = "Words in WJ3 only")

```

Inspecting those words in the WJ3 set are learned at least somewhat by the final epoch:

```{r}
d %>% 
  filter(!in_traindata & in_validation_set) %>%
  filter(correct == 1) %>% 
  select(epoch, word) %>% 
  knitr::kable()

```

If we calculate accuracy for each word across all epochs and rank order by the aggregate accuracy, you see a skewed distribution. Words in the upper portion are not in the training set or are words that are never read correctly.

```{r}
d %>% 
  group_by(word) %>% 
  summarise(correct = mean(correct)) %>% 
  arrange(desc(correct)) %>% 
  mutate(rank = seq_len(n())) %>% 
  ggplot(aes(correct, reorder(word, rank))) +
  geom_bar(stat = "identity") +
  labs(title = "Mean correct per word (across epochs)", y = "Word (arranged by mean correct)", x = "Correct (as proportion)") +
  theme_apa() +
  theme(axis.text.y = element_text(size = 8))

```
Let's identify the words that are in the training set that are never learned (at least in the dichotomous measure):

```{r}
words_never_learned = d %>% 
  filter(in_traindata) %>% 
  group_by(word) %>% 
  summarise(accuracy = sum(correct)) %>% 
  filter(accuracy == 0) %>% 
  pull(word)

```
There are `r length(words_never_learned)` words in the training set that are never read correctly. Let's look at the performance on these words as a function of frequency and consistency.

Here we see that words that are never learned are in fact less consistent (using the Chee et al., 2020 norms) and less frequent (using the frequency data implemented in training).

```{r}
plot_a = d %>% 
  mutate(never_learned = case_when(word %in% words_never_learned ~ "Never learned",
                                   TRUE ~ "Learned")) %>% 
  filter(epoch == 20) %>% 
  ggplot(aes(factor(never_learned), consistency)) +
  geom_boxplot() +
  labs(x = "Learn status", title = "Consistency", y = "Consistency") +
  theme_apa()

plot_b = d %>% 
  mutate(never_learned = case_when(word %in% words_never_learned ~ "Never learned",
                                   TRUE ~ "Learned")) %>% 
  filter(epoch == 20) %>% 
  mutate(f = case_when(is.na(f) ~ 2, # k-smoothed to avoid issues with log() and smooth()
                       TRUE ~ f + 2)) %>%
  ggplot(aes(factor(never_learned), log(f))) +
  geom_boxplot() +
  labs(x = "Learn status", title = "Frequency", y = "Frequency") +
  theme_apa()

cowplot::plot_grid(plot_a, plot_b)  
```

These results suggest the potential value in looking at accuracy as a graded measure, rather than binary (we could use MSE for this).

## Accuracy as a function of program type
Words vary in terms of their presence (and frequency) across programs. The data below capture aspects of learning as a function of the frequency within each of the programs.

First, the aggregate accuracy in the two program types across training for words that are present in that program type. This expresses how each of the programs do on their own words throughout development.


```{r AccuracyByProgramType}
plot_data = programs %>% 
                filter(!is.na(program_type) & in_traindata) %>% 
                group_by(epoch, program_type) %>% 
                summarise(sd_ = sd(correct, na.rm = T),
                          correct = mean(correct),
                          mean_value = mean(correct),
                          n = n(),
                          se_value = sd_/sqrt(n))


programs %>% 
  filter(f > 0) %>% 
  ggplot(aes(epoch, correct, color = program_type, group = program_type)) +
  geom_line(stat = "summary") +
  geom_ribbon(data = plot_data, aes(x = epoch, ymin = mean_value - se_value, ymax = mean_value + se_value), alpha = 0.3) +
  geom_point(stat = "summary") +
  labs(x = "Epoch", y = "Accuracy", color = "Program type") +
  theme_apa()

```

Based on these data, the leveled texts are achieving an overall higher average accuracy across training items. It is important to remember though that words can appear in different quantities in each of the programs.

### Learning WJ3 words across epochs
Let's look at these learning curves, but only on the test words from WJ3. This is simply measuring whether or not the program type contains the WJ3 words included in each program type. Nonetheless, these curves are basically undifferentiated (because almost all the WJ3 words that are in one program type are in the other).

```{r}
plot_data = programs %>% 
                filter(!is.na(program_type) & in_traindata & in_validation_set) %>% 
                group_by(epoch, program_type) %>% 
                summarise(sd_ = sd(correct, na.rm = T),
                          correct = mean(correct),
                          mean_value = mean(correct),
                          n = n(),
                          se_value = sd_/sqrt(n))


programs %>% 
  filter(f > 0) %>% 
  filter(!is.na(program_type) & in_traindata & in_validation_set) %>% 
  ggplot(aes(epoch, correct, color = program_type, group = program_type)) +
  geom_line(stat = "summary") +
  #geom_ribbon(data = plot_data, aes(x = epoch, ymin = mean_value - se_value, ymax = mean_value + se_value), alpha = 0.3) +
  labs(x = "Epoch", y = "Accuracy", color = "Program type") +
  theme_apa()

```

### Quantities of words in programs
```{r WordsUniqueToEachProgram}
words_in_decodable = programs %>% 
  filter(program_type == 'Decodable') %>% 
  distinct(word) %>% 
  pull(word)

  
words_in_leveled = programs %>% 
  filter(program_type == 'Leveled') %>% 
  distinct(word) %>% 
  pull(word)

words_unique_to_decodable = words_in_decodable[!(words_in_decodable %in% words_in_leveled)]

words_unique_to_leveled = words_in_leveled[!(words_in_leveled %in% words_in_decodable)]

```


```{r WordsByProgramType}
plot_data = programs %>% 
                filter(!is.na(program_type) & epoch == 20) %>% 
                group_by(epoch, program_type) %>% 
                summarise(sd_ = sd(f, na.rm = T),
                          f = mean(f),
                          mean_value = mean(f),
                          n = n(),
                          se_value = sd_/sqrt(n))


plot_a = programs %>% 
  filter(f > 0 & epoch == 20) %>% 
  ggplot(aes(program_type, f, fill = program_type)) +
  geom_bar(stat = "summary", color = "black") +
  geom_errorbar(data = plot_data, aes(x = program_type, ymin = mean_value - se_value, ymax = mean_value + se_value), width = .25, color = "black") +
  labs(x = "Program type", y = "Average frequency per word") +
  theme_apa() +
  theme(legend.position = "None",
        axis.title.y = element_text(size = 10))


plot_b = programs %>% 
  filter(f > 0 & epoch == 20) %>% 
  group_by(program_type) %>% 
  summarise(f = sum(f)) %>%
  ggplot(aes(program_type, f, fill = program_type)) +
  geom_bar(stat = "identity", color = "black") +
  labs(x = "Program type", y = "Total words") +
  theme_apa() +
  theme(legend.position = "None")

plot_c = programs %>% 
  filter(!is.na(program_type)) %>% 
  group_by(program_type) %>% 
  distinct(word) %>% 
  summarise(f = n()) %>% 
  ggplot(aes(program_type, f, fill = program_type)) +
  geom_bar(stat = "summary", color = "black") +
  labs(x = "Program type", y = "Number of unique words") +
  theme_apa() +
  theme(legend.position = "None")

plot_d = tibble(program_type = c("Decodable", "Leveled"),
                q = c(length(words_unique_to_decodable), length(words_unique_to_leveled))) %>% 
  ggplot(aes(program_type, q, fill = program_type)) +
  geom_bar(stat = "identity", color = "black") +
  labs(x = "Program type", y = "Words unique to the program") +
  theme_apa() +
  theme(legend.position = "None")

  

plot_grid(plot_a, plot_b, plot_c, plot_d)
```




## Frequency of words in programs with frequency norms
One question concerns the extent to which the frequency with which words appear in programs correlate with their frequency in other, established norms. The plot below breaks this out by program type to give you a sense as to whether or not this correlation differs as a function of the types of programs we are using.



```{r}
programs %>% 
  filter(!is.na(program_type)) %>% 
  left_join(tasa) %>% 
  filter(epoch == 20) %>% 
  mutate(f = case_when(is.na(f) ~ 2,
                       TRUE ~ f + 2)) %>%
  ggplot(aes(log(f), log(tasa), color = program_type)) +
  geom_point(size = .5, color = "grey16") +
  geom_smooth() +
  facet_grid(vars(program_type)) +
  labs(x = "Frequency in programs (log)", y = "Frequency in TASA (log)", color = "Program type") +
  theme_apa()

```

And for words that are present in both programs, are their frequencies correlated? Pretty clearly, yes - they are correlated.

```{r}
programs %>% 
  filter(epoch == 20) %>% 
  filter(word %nin% words_unique_to_decodable & word %nin% words_unique_to_leveled) %>% 
  filter(program_type == "Decodable") %>% 
  select(word, decodable = f) %>% 
  left_join(programs %>% 
    filter(epoch == 20) %>% 
    filter(word %nin% words_unique_to_decodable & word %nin% words_unique_to_leveled) %>% 
    filter(program_type == "Leveled") %>% 
    select(word, leveled = f)) %>% 
  ggplot(aes(log(decodable), log(leveled))) +
  geom_point() +
  geom_smooth(color = "Firebrick4") +
  labs(x = "Frequency (log) of words in decodable programs", y = "Frequency of words in leveled program",
       title = "Frequency (log) of words common to both types of programs") +
  theme_apa()



```


Let's look at how frequency within each program type is associated with accuracy across each epoch. This shows how accuracy is changing across epochs based on the frequency of words in each program type. The frequency effect becomes increasingly minimal across training.

```{r}
plot_a = programs %>% 
  filter(program_type == "Decodable") %>% 
  ggplot(aes(factor(correct), f)) +
  geom_bar(stat = "summary") +
  facet_wrap(vars(epoch), ncol = 5) +
  labs(x = "Accuracy", y = "Frequency", title = "Decodable Texts") +
  theme_apa()

plot_b = programs %>% 
  filter(program_type == "Leveled") %>% 
  ggplot(aes(factor(correct), f)) +
  geom_bar(stat = "summary") +
  facet_wrap(vars(epoch), ncol = 5) +
  labs(x = "Accuracy", y = "Frequency", title = "Leveled Texts") +
  theme_apa()

plot_grid(plot_a, plot_b)
```


## Accuracy of words unique to one program or the other

```{r AccuracyOfWordsUniqueToProgramsData}
plot_data = programs %>% 
  filter(in_traindata &!is.na(program_type)) %>% 
  mutate(condition = case_when(word %in% words_unique_to_decodable ~ "Decodable",
                               word %in% words_unique_to_leveled ~ "Leveled",
                               TRUE ~ "Decodable & Leveled")) %>% 
    group_by(epoch, condition) %>% 
    summarise(sd_ = sd(correct, na.rm = T),
              correct = mean(correct),
              mean_value = mean(correct),
              n = n(),
              se_value = sd_/sqrt(n))

plot_data %>% 
  filter(epoch == 20) %>% 
  select(condition, n)
```

The model achieves consistently better performance on the words that are in the decodable programs relative to the other, despite the fact that the decodable programs have any many more words contained in them. However, the model does best (and performs adequately) on words that are shared by both programs (i.e., the most generally useful words). Note however, that the quantity of words that are present in _both_ programs is only slightly greater (`r plot_data %>% filter(epoch == 20 & condition == "Decodable & Leveled") %>% pull(n)`) than the number of words in just the decodable ones (`r plot_data %>% filter(epoch == 20 & condition == "Decodable") %>% pull(n)`), but is vastly more (around 6.5x) than the number of words only in the leveled program (`r plot_data %>% filter(epoch == 20 & condition == "Leveled") %>% pull(n)`).

```{r AccuracyOfWordsUniqueToPrograms}
programs %>% 
  #filter(word %in% c(words_unique_to_decodable, words_unique_to_leveled)) %>% 
  filter(in_traindata &!is.na(program_type)) %>% 
  mutate(condition = case_when(word %in% words_unique_to_decodable ~ "Decodable",
                               word %in% words_unique_to_leveled ~ "Leveled",
                               TRUE ~ "Decodable & Leveled")) %>% 
  ggplot(aes(epoch, correct, color = condition, group = condition)) +
  geom_line(stat = "summary") +
  geom_ribbon(data = plot_data, aes(x = epoch, ymin = mean_value - se_value, ymax = mean_value + se_value), alpha = 0.3) +
  geom_point(stat = "summary") +
  labs(x = "Epoch", y = "Accuracy", color = "Program type", title = "Accuracy on words unique to each program type") +
  theme_apa()

```


## The sequence in which words are learned and their relationship to WJ3
The WJ3 test sets out a specific sequence of learning for the words (and letters) have been normed in their test. In our model we can examine an analogous property by looking at when the word is pronounced correctly for the first time.

```{r LearnabilityMeasures}
first_learned = d %>% 
  filter(correct == 1) %>% 
  group_by(word) %>% 
  arrange(epoch) %>% 
  summarise(in_traindata = first(in_traindata),
            in_validation_set = first(in_validation_set),
            first_learned = min(epoch))

  
wj3_sequence %>% 
  left_join(first_learned %>% 
              select(word, first_learned)) %>%
  #filter(in_traindata) %>%
  ggplot(aes(sequence, factor(first_learned), label = word, fill = in_traindata)) +
  geom_label(position = position_jitter(height = .5)) +
  theme_apa() +
  labs(x = "WJ3 sequence", y = "First learned by model") +
  theme(legend.position = "none")

```



```{r}
wj3_sequence %>% 
  left_join(d %>% 
              filter(epoch == 20) %>% 
              select(word, phon_distance_from_target)) %>%
  ggplot(aes(sequence, phon_distance_from_target)) +
  geom_point()


```







## Accuracy for individual programs (rather than types of program)


## Stress pattern



## Phonemes
## Vowels: schwa

```{r}

words %>% 
  group_by(epoch, word) %>% 
  summarise(accuracy = mean(correct),
            in_validation_set = first(in_validation_set)) %>% 
  ggplot(aes(epoch, accuracy)) +
  geom_bar(stat = "identity") +
  geom_smooth() +
  facet_grid(in_validation_set)

```

## SSSR 2024 Abstract
```{r SSSR2024, child = "SSSR2024.Rmd"}


correlation = wj3_sequence %>% 
  left_join(first_learned %>% 
              select(word, first_learned)) %>%
  filter(in_traindata) %>%
  filter(!is.na(first_learned)) %>%
  summarise(rho = cor(sequence, first_learned, method = "spearman"))

```

## STE Talk by MCB on 12.1.23

```{r child="STE2023.Rmd"}


```

## What we need to do next
We need to include more words in training and implement a true holdout set (could include holding out WJ3 completely).